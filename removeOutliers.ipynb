{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980af0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykalman import KalmanFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ddf7c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Demand_with_Temperature.csv...\n",
      "Loading tmp/Outage_Runs.csv...\n",
      "Demand data shape: (1157172, 17)\n",
      "Outage periods: 31 entries\n",
      "Date range: 2013-12-26 01:00:00 to 2024-12-26 00:00:00\n",
      "Loading tmp/Outage_Runs.csv...\n",
      "Demand data shape: (1157172, 17)\n",
      "Outage periods: 31 entries\n",
      "Date range: 2013-12-26 01:00:00 to 2024-12-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load the main dataset and outage periods\n",
    "def load_data():\n",
    "    \"\"\"Load the demand data and outage periods\"\"\"\n",
    "    print(\"Loading Demand_with_Temperature.csv...\")\n",
    "    demand_df = pd.read_csv('Demand_with_Temperature.csv')\n",
    "    demand_df['DateTime'] = pd.to_datetime(demand_df['DateTime'])\n",
    "    \n",
    "    print(\"Loading tmp/Outage_Runs.csv...\")\n",
    "    outage_df = pd.read_csv('tmp/Outage_Runs.csv')\n",
    "    outage_df['start'] = pd.to_datetime(outage_df['start'])\n",
    "    outage_df['end'] = pd.to_datetime(outage_df['end'])\n",
    "    \n",
    "    print(f\"Demand data shape: {demand_df.shape}\")\n",
    "    print(f\"Outage periods: {len(outage_df)} entries\")\n",
    "    print(f\"Date range: {demand_df['DateTime'].min()} to {demand_df['DateTime'].max()}\")\n",
    "    \n",
    "    return demand_df, outage_df\n",
    "\n",
    "# Load the data\n",
    "demand_df, outage_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa798129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying outlier periods...\n",
      "Outage 1: 2014-03-20 07:05:00 to 2014-03-20 18:00:00 - 132 points marked as outliers\n",
      "Outage 2: 2014-07-10 10:00:00 to 2014-07-10 16:55:00 - 84 points marked as outliers\n",
      "Outage 3: 2014-07-16 05:00:00 to 2014-07-18 17:55:00 - 732 points marked as outliers\n",
      "Outage 4: 2015-02-07 02:05:00 to 2015-02-07 10:00:00 - 96 points marked as outliers\n",
      "Outage 5: 2015-08-23 01:00:00 to 2015-08-23 07:55:00 - 84 points marked as outliers\n",
      "Outage 6: 2015-11-23 17:00:00 to 2015-11-23 17:55:00 - 12 points marked as outliers\n",
      "Outage 7: 2015-11-27 04:00:00 to 2015-11-27 04:55:00 - 12 points marked as outliers\n",
      "Outage 8: 2016-03-19 11:05:00 to 2016-03-19 17:00:00 - 72 points marked as outliers\n",
      "Outage 9: 2016-03-30 12:05:00 to 2016-03-30 15:00:00 - 36 points marked as outliers\n",
      "Outage 10: 2016-12-25 00:00:00 to 2016-12-25 23:55:00 - 288 points marked as outliers\n",
      "Outage 11: 2016-12-26 12:05:00 to 2016-12-26 14:00:00 - 24 points marked as outliers\n",
      "Outage 12: 2017-04-20 00:00:00 to 2017-04-20 01:55:00 - 24 points marked as outliers\n",
      "Outage 13: 2018-01-17 08:05:00 to 2018-01-17 18:00:00 - 120 points marked as outliers\n",
      "Outage 14: 2018-08-10 23:00:00 to 2018-08-11 02:55:00 - 48 points marked as outliers\n",
      "Outage 15: 2018-12-13 06:00:00 to 2018-12-13 20:55:00 - 180 points marked as outliers\n",
      "Outage 16: 2019-02-19 06:05:00 to 2019-02-19 23:00:00 - 204 points marked as outliers\n",
      "Outage 17: 2019-10-18 09:00:00 to 2019-10-18 10:55:00 - 24 points marked as outliers\n",
      "Outage 18: 2020-01-24 16:05:00 to 2020-01-24 17:00:00 - 12 points marked as outliers\n",
      "Outage 19: 2020-04-02 14:05:00 to 2020-04-02 15:00:00 - 12 points marked as outliers\n",
      "Outage 20: 2020-09-19 08:00:00 to 2020-09-19 17:55:00 - 120 points marked as outliers\n",
      "Outage 21: 2020-10-26 09:00:00 to 2020-10-26 11:55:00 - 36 points marked as outliers\n",
      "Outage 22: 2020-11-28 06:00:00 to 2020-11-28 16:55:00 - 132 points marked as outliers\n",
      "Outage 23: 2021-04-10 08:00:00 to 2021-04-10 16:55:00 - 108 points marked as outliers\n",
      "Outage 24: 2021-09-08 13:00:00 to 2021-09-08 16:55:00 - 48 points marked as outliers\n",
      "Outage 25: 2022-02-26 13:25:00 to 2022-02-26 16:30:00 - 38 points marked as outliers\n",
      "Outage 26: 2022-06-12 06:35:00 to 2022-06-12 19:45:00 - 159 points marked as outliers\n",
      "Outage 27: 2022-06-12 20:35:00 to 2022-06-12 21:20:00 - 10 points marked as outliers\n",
      "Outage 28: 2023-01-21 06:35:00 to 2023-01-21 17:00:00 - 126 points marked as outliers\n",
      "Outage 29: 2023-03-25 06:25:00 to 2023-03-25 18:05:00 - 141 points marked as outliers\n",
      "Outage 30: 2024-06-21 07:10:00 to 2024-06-21 09:55:00 - 34 points marked as outliers\n",
      "Outage 31: 2024-10-24 04:00:00 to 2024-10-24 19:35:00 - 188 points marked as outliers\n",
      "\n",
      "Total outlier points identified: 3336 (0.29%)\n",
      "Outage 27: 2022-06-12 20:35:00 to 2022-06-12 21:20:00 - 10 points marked as outliers\n",
      "Outage 28: 2023-01-21 06:35:00 to 2023-01-21 17:00:00 - 126 points marked as outliers\n",
      "Outage 29: 2023-03-25 06:25:00 to 2023-03-25 18:05:00 - 141 points marked as outliers\n",
      "Outage 30: 2024-06-21 07:10:00 to 2024-06-21 09:55:00 - 34 points marked as outliers\n",
      "Outage 31: 2024-10-24 04:00:00 to 2024-10-24 19:35:00 - 188 points marked as outliers\n",
      "\n",
      "Total outlier points identified: 3336 (0.29%)\n"
     ]
    }
   ],
   "source": [
    "# Function to identify outlier periods\n",
    "def identify_outlier_periods(demand_df, outage_df):\n",
    "    \"\"\"Mark all data points that fall within outage periods as outliers\"\"\"\n",
    "    demand_df = demand_df.copy()\n",
    "    demand_df['is_outlier'] = False\n",
    "    \n",
    "    print(\"Identifying outlier periods...\")\n",
    "    for idx, row in outage_df.iterrows():\n",
    "        start_time = row['start']\n",
    "        end_time = row['end']\n",
    "        \n",
    "        # Mark points within this outage period as outliers\n",
    "        mask = (demand_df['DateTime'] >= start_time) & (demand_df['DateTime'] <= end_time)\n",
    "        outlier_count = mask.sum()\n",
    "        demand_df.loc[mask, 'is_outlier'] = True\n",
    "        \n",
    "        print(f\"Outage {idx+1}: {start_time} to {end_time} - {outlier_count} points marked as outliers\")\n",
    "    \n",
    "    total_outliers = demand_df['is_outlier'].sum()\n",
    "    print(f\"\\nTotal outlier points identified: {total_outliers} ({total_outliers/len(demand_df)*100:.2f}%)\")\n",
    "    \n",
    "    return demand_df\n",
    "\n",
    "# Identify outlier periods\n",
    "demand_df = identify_outlier_periods(demand_df, outage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2059c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Kalman filtering...\n",
      "Fitting Kalman Filter using EM algorithm...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m smoothed_values\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Apply Kalman smoothing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m smoothed_demand = \u001b[43mapply_kalman_smoothing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdemand_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTOTAL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemand_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mis_outlier\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mapply_kalman_smoothing\u001b[39m\u001b[34m(demand_series, outlier_mask)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Fit the model using EM algorithm on non-outlier data\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFitting Kalman Filter using EM algorithm...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m kf = \u001b[43mkf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Apply smoothing to get the best estimates\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mApplying Kalman smoothing...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/dev/Classical Forecasting/wsl-venv/lib/python3.12/site-packages/pykalman/standard.py:1486\u001b[39m, in \u001b[36mKalmanFilter.em\u001b[39m\u001b[34m(self, X, y, n_iter, em_vars)\u001b[39m\n\u001b[32m   1478\u001b[39m \u001b[38;5;66;03m# Actual EM iterations\u001b[39;00m\n\u001b[32m   1479\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[32m   1480\u001b[39m     (\n\u001b[32m   1481\u001b[39m         predicted_state_means,\n\u001b[32m   1482\u001b[39m         predicted_state_covariances,\n\u001b[32m   1483\u001b[39m         kalman_gains,\n\u001b[32m   1484\u001b[39m         filtered_state_means,\n\u001b[32m   1485\u001b[39m         filtered_state_covariances,\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m     ) = \u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransition_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransition_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransition_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_state_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_state_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1497\u001b[39m     (\n\u001b[32m   1498\u001b[39m         smoothed_state_means,\n\u001b[32m   1499\u001b[39m         smoothed_state_covariances,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1506\u001b[39m         predicted_state_covariances,\n\u001b[32m   1507\u001b[39m     )\n\u001b[32m   1508\u001b[39m     sigma_pair_smooth = _smooth_pair(\n\u001b[32m   1509\u001b[39m         smoothed_state_covariances, kalman_smoothing_gains\n\u001b[32m   1510\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/dev/Classical Forecasting/wsl-venv/lib/python3.12/site-packages/pykalman/standard.py:402\u001b[39m, in \u001b[36m_filter\u001b[39m\u001b[34m(transition_matrices, observation_matrices, transition_covariance, observation_covariance, transition_offsets, observation_offsets, initial_state_mean, initial_state_covariance, observations)\u001b[39m\n\u001b[32m    396\u001b[39m     observation_covariance_t = _last_dims(observation_covariance, t)\n\u001b[32m    397\u001b[39m     observation_offset = _last_dims(observation_offsets, t, ndims=\u001b[32m1\u001b[39m)\n\u001b[32m    398\u001b[39m     (\n\u001b[32m    399\u001b[39m         kalman_gains[t],\n\u001b[32m    400\u001b[39m         filtered_state_means[t],\n\u001b[32m    401\u001b[39m         filtered_state_covariances[t],\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     ) = \u001b[43m_filter_correct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_covariance_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicted_state_means\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicted_state_covariances\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    412\u001b[39m     predicted_state_means,\n\u001b[32m    413\u001b[39m     predicted_state_covariances,\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     filtered_state_covariances,\n\u001b[32m    417\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/dev/Classical Forecasting/wsl-venv/lib/python3.12/site-packages/pykalman/standard.py:288\u001b[39m, in \u001b[36m_filter_correct\u001b[39m\u001b[34m(observation_matrix, observation_covariance, observation_offset, predicted_state_mean, predicted_state_covariance, observation)\u001b[39m\n\u001b[32m    275\u001b[39m predicted_observation_mean = (\n\u001b[32m    276\u001b[39m     np.dot(observation_matrix, predicted_state_mean) + observation_offset\n\u001b[32m    277\u001b[39m )\n\u001b[32m    278\u001b[39m predicted_observation_covariance = (\n\u001b[32m    279\u001b[39m     np.dot(\n\u001b[32m    280\u001b[39m         observation_matrix,\n\u001b[32m   (...)\u001b[39m\u001b[32m    283\u001b[39m     + observation_covariance\n\u001b[32m    284\u001b[39m )\n\u001b[32m    286\u001b[39m kalman_gain = np.dot(\n\u001b[32m    287\u001b[39m     predicted_state_covariance,\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     np.dot(observation_matrix.T, \u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_observation_covariance\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[32m    289\u001b[39m )\n\u001b[32m    291\u001b[39m corrected_state_mean = predicted_state_mean + np.dot(\n\u001b[32m    292\u001b[39m     kalman_gain, observation - predicted_observation_mean\n\u001b[32m    293\u001b[39m )\n\u001b[32m    294\u001b[39m corrected_state_covariance = predicted_state_covariance - np.dot(\n\u001b[32m    295\u001b[39m     kalman_gain, np.dot(observation_matrix, predicted_state_covariance)\n\u001b[32m    296\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/dev/Classical Forecasting/wsl-venv/lib/python3.12/site-packages/scipy/linalg/_basic.py:1606\u001b[39m, in \u001b[36mpinv\u001b[39m\u001b[34m(a, atol, rtol, return_rank, check_finite)\u001b[39m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpinv\u001b[39m(a, *, atol=\u001b[38;5;28;01mNone\u001b[39;00m, rtol=\u001b[38;5;28;01mNone\u001b[39;00m, return_rank=\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1508\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1509\u001b[39m \u001b[33;03m    Compute the (Moore-Penrose) pseudo-inverse of a matrix.\u001b[39;00m\n\u001b[32m   1510\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1604\u001b[39m \n\u001b[32m   1605\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1606\u001b[39m     a = \u001b[43m_asarray_validated\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1607\u001b[39m     u, s, vh = _decomp_svd.svd(a, full_matrices=\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1608\u001b[39m     t = u.dtype.char.lower()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/dev/Classical Forecasting/wsl-venv/lib/python3.12/site-packages/scipy/_lib/_util.py:537\u001b[39m, in \u001b[36m_asarray_validated\u001b[39m\u001b[34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[39m\n\u001b[32m    535\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mmasked arrays are not supported\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    536\u001b[39m toarray = np.asarray_chkfinite \u001b[38;5;28;01mif\u001b[39;00m check_finite \u001b[38;5;28;01melse\u001b[39;00m np.asarray\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m a = \u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m objects_ok:\n\u001b[32m    539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m a.dtype \u001b[38;5;129;01mis\u001b[39;00m np.dtype(\u001b[33m'\u001b[39m\u001b[33mO\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/dev/Classical Forecasting/wsl-venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:645\u001b[39m, in \u001b[36masarray_chkfinite\u001b[39m\u001b[34m(a, dtype, order)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert the input to an array, checking for NaNs or Infs.\u001b[39;00m\n\u001b[32m    582\u001b[39m \n\u001b[32m    583\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    642\u001b[39m \n\u001b[32m    643\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    644\u001b[39m a = asarray(a, dtype=dtype, order=order)\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m a.dtype.char \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[33m'\u001b[39m\u001b[33mAllFloat\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    647\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33marray must not contain infs or NaNs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/dev/Classical Forecasting/wsl-venv/lib/python3.12/site-packages/numpy/_core/_methods.py:73\u001b[39m, in \u001b[36m_all\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m umr_all(a, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Kalman Filter implementation for smoothing\n",
    "def apply_kalman_smoothing(demand_series, outlier_mask):\n",
    "    \"\"\"Apply Kalman filtering to smooth the demand data and replace outliers\"\"\"\n",
    "    print(\"Applying Kalman filtering...\")\n",
    "    \n",
    "    # Prepare the data - use observations where we don't have outliers\n",
    "    observations = demand_series.values.copy()\n",
    "    \n",
    "    # Create a masked array where outliers are marked as missing\n",
    "    masked_observations = np.ma.masked_array(observations, mask=outlier_mask)\n",
    "    \n",
    "    # Set up Kalman Filter\n",
    "    # State transition matrix (assumes trend continues)\n",
    "    transition_matrix = np.array([[1, 1], [0, 1]])\n",
    "    \n",
    "    # Observation matrix (we observe the level)\n",
    "    observation_matrix = np.array([[1, 0]])\n",
    "    \n",
    "    # Initialize Kalman Filter\n",
    "    kf = KalmanFilter(\n",
    "        transition_matrices=transition_matrix,\n",
    "        observation_matrices=observation_matrix,\n",
    "        initial_state_mean=[observations[0], 0],  # Start with first observation and zero trend\n",
    "        n_dim_state=2\n",
    "    )\n",
    "    \n",
    "    # Fit the model using EM algorithm on non-outlier data\n",
    "    print(\"Fitting Kalman Filter using EM algorithm...\")\n",
    "    kf = kf.em(masked_observations, n_iter=50)\n",
    "    \n",
    "    # Apply smoothing to get the best estimates\n",
    "    print(\"Applying Kalman smoothing...\")\n",
    "    state_means, state_covariances = kf.smooth(masked_observations)\n",
    "    \n",
    "    # Extract the smoothed values (level component)\n",
    "    smoothed_values = state_means[:, 0]\n",
    "    \n",
    "    return smoothed_values\n",
    "\n",
    "# Apply Kalman smoothing\n",
    "smoothed_demand = apply_kalman_smoothing(demand_df['TOTAL'], demand_df['is_outlier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18841814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cleaned dataset\n",
    "def create_cleaned_dataset(demand_df, smoothed_demand):\n",
    "    \"\"\"Create the final cleaned dataset with outliers replaced by Kalman smoothed values\"\"\"\n",
    "    cleaned_df = demand_df.copy()\n",
    "    \n",
    "    # Replace outlier values with smoothed values\n",
    "    cleaned_df['TOTAL_original'] = cleaned_df['TOTAL'].copy()\n",
    "    cleaned_df.loc[cleaned_df['is_outlier'], 'TOTAL'] = smoothed_demand[cleaned_df['is_outlier']]\n",
    "    \n",
    "    # Add a column to track which values were smoothed\n",
    "    cleaned_df['smoothed'] = cleaned_df['is_outlier'].copy()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    outlier_count = cleaned_df['is_outlier'].sum()\n",
    "    print(f\"Replaced {outlier_count} outlier values with Kalman smoothed estimates\")\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "# Create cleaned dataset\n",
    "cleaned_df = create_cleaned_dataset(demand_df, smoothed_demand)\n",
    "\n",
    "# Display some statistics\n",
    "print(\"\\nCleaning Statistics:\")\n",
    "print(f\"Total data points: {len(cleaned_df)}\")\n",
    "print(f\"Outlier points replaced: {cleaned_df['is_outlier'].sum()}\")\n",
    "print(f\"Percentage of data smoothed: {cleaned_df['is_outlier'].sum()/len(cleaned_df)*100:.2f}%\")\n",
    "\n",
    "# Show sample of cleaned data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(cleaned_df[['DateTime', 'TOTAL_original', 'TOTAL', 'smoothed', 'Temperature']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization to show the effect of outlier removal\n",
    "def plot_comparison(cleaned_df, sample_start='2014-03-19', sample_end='2014-03-21'):\n",
    "    \"\"\"Plot comparison between original and cleaned data for a sample period\"\"\"\n",
    "    \n",
    "    # Filter for sample period around one of the outages\n",
    "    sample_mask = (cleaned_df['DateTime'] >= sample_start) & (cleaned_df['DateTime'] <= sample_end)\n",
    "    sample_df = cleaned_df[sample_mask].copy()\n",
    "    \n",
    "    if len(sample_df) == 0:\n",
    "        print(f\"No data found for period {sample_start} to {sample_end}\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot original data\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(sample_df['DateTime'], sample_df['TOTAL_original'], 'b-', label='Original Data', alpha=0.7)\n",
    "    outlier_mask = sample_df['is_outlier']\n",
    "    if outlier_mask.any():\n",
    "        plt.scatter(sample_df[outlier_mask]['DateTime'], \n",
    "                   sample_df[outlier_mask]['TOTAL_original'], \n",
    "                   color='red', s=20, label='Outliers', zorder=5)\n",
    "    plt.title('Original Data with Outliers Highlighted')\n",
    "    plt.ylabel('Demand (TOTAL)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot cleaned data\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(sample_df['DateTime'], sample_df['TOTAL'], 'g-', label='Cleaned Data (Kalman Smoothed)', linewidth=2)\n",
    "    if outlier_mask.any():\n",
    "        plt.scatter(sample_df[outlier_mask]['DateTime'], \n",
    "                   sample_df[outlier_mask]['TOTAL'], \n",
    "                   color='orange', s=20, label='Smoothed Values', zorder=5)\n",
    "    plt.title('Cleaned Data with Kalman Smoothed Values')\n",
    "    plt.ylabel('Demand (TOTAL)')\n",
    "    plt.xlabel('DateTime')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Sample period: {sample_start} to {sample_end}\")\n",
    "    print(f\"Data points in sample: {len(sample_df)}\")\n",
    "    print(f\"Outlier points in sample: {outlier_mask.sum()}\")\n",
    "\n",
    "# Create visualization\n",
    "plot_comparison(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e70fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "def save_cleaned_data(cleaned_df, filename='Demand_smoothed_kalman.csv'):\n",
    "    \"\"\"Save the cleaned dataset to CSV file with same structure as original\"\"\"\n",
    "    \n",
    "    # Get original column order from the input dataset\n",
    "    original_columns = ['DateTime','TOTAL','Outage','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday','Holiday','ECQ','GCQ','ALERT LEVEL 1','ALERT LEVEL 2','ALERT LEVEL 3','Temperature']\n",
    "    \n",
    "    # Prepare final dataset - keep only original columns with cleaned TOTAL values\n",
    "    final_df = cleaned_df[original_columns].copy()\n",
    "    \n",
    "    # Save to CSV with exact same structure as original\n",
    "    final_df.to_csv(filename, index=False)\n",
    "    print(f\"Cleaned dataset saved to: {filename}\")\n",
    "    print(f\"Shape: {final_df.shape}\")\n",
    "    print(f\"Columns: {list(final_df.columns)}\")\n",
    "    print(f\"Structure matches original Demand_with_Temperature.csv\")\n",
    "    \n",
    "    # Show how many values were actually replaced\n",
    "    outlier_count = cleaned_df['is_outlier'].sum()\n",
    "    print(f\"Values replaced with Kalman smoothing: {outlier_count}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Save the cleaned dataset\n",
    "final_cleaned_df = save_cleaned_data(cleaned_df)\n",
    "\n",
    "print(\"\\nFirst few rows of saved dataset:\")\n",
    "print(final_cleaned_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and validation\n",
    "def generate_summary(original_df, cleaned_df, outage_df):\n",
    "    \"\"\"Generate summary statistics about the outlier removal process\"\"\"\n",
    "    \n",
    "    print(\"=== OUTLIER REMOVAL SUMMARY ===\")\n",
    "    print(f\"Original dataset shape: {original_df.shape}\")\n",
    "    print(f\"Cleaned dataset shape: {cleaned_df.shape}\")\n",
    "    print(f\"Number of outage periods processed: {len(outage_df)}\")\n",
    "    \n",
    "    # Calculate outlier statistics\n",
    "    total_outliers = cleaned_df['smoothed'].sum() if 'smoothed' in cleaned_df.columns else 0\n",
    "    print(f\"\\nOutlier Statistics:\")\n",
    "    print(f\"Total outlier points identified: {total_outliers}\")\n",
    "    print(f\"Percentage of data smoothed: {total_outliers/len(cleaned_df)*100:.2f}%\")\n",
    "    \n",
    "    # Show range of smoothed values vs original\n",
    "    if 'TOTAL_original' in cleaned_df.columns:\n",
    "        smoothed_mask = cleaned_df['smoothed'] == True\n",
    "        if smoothed_mask.any():\n",
    "            original_outlier_values = cleaned_df[smoothed_mask]['TOTAL_original']\n",
    "            smoothed_values = cleaned_df[smoothed_mask]['TOTAL']\n",
    "            \n",
    "            print(f\"\\nValue Comparison for Smoothed Points:\")\n",
    "            print(f\"Original outlier values - Min: {original_outlier_values.min():.2f}, Max: {original_outlier_values.max():.2f}, Mean: {original_outlier_values.mean():.2f}\")\n",
    "            print(f\"Smoothed values - Min: {smoothed_values.min():.2f}, Max: {smoothed_values.max():.2f}, Mean: {smoothed_values.mean():.2f}\")\n",
    "    \n",
    "    # Show some examples of outage periods that were processed\n",
    "    print(f\"\\nOutage Periods Processed:\")\n",
    "    for idx, row in outage_df.head().iterrows():\n",
    "        duration = row['duration_hours']\n",
    "        print(f\"  {idx+1}. {row['start']} to {row['end']} ({duration:.1f} hours)\")\n",
    "    \n",
    "    if len(outage_df) > 5:\n",
    "        print(f\"  ... and {len(outage_df)-5} more periods\")\n",
    "    \n",
    "    print(f\"\\nCleaned dataset saved as: 'Demand_smoothed_kalman.csv'\")\n",
    "    print(\"Process completed successfully!\")\n",
    "\n",
    "# Generate final summary\n",
    "generate_summary(demand_df, final_cleaned_df, outage_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl-venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
